"""
This file contains the  methods to verify the following Bluestore features.
1.Checking the OSD memory with the heap dump
2.Checking the OSD memory growth with the dump mem pools
"""

import datetime
import time

from ceph.ceph_admin import CephAdmin
from ceph.rados.core_workflows import RadosOrchestrator
from ceph.rados.rados_scrub import RadosScrubber
from tests.rados.monitor_configurations import MonConfigMethods
from utility.log import Log
from utility.utils import method_should_succeed

log = Log(__name__)


def run(ceph_cluster, **kw):
    """
    Performs the Bluestore related tests.
    Returns:
        1 -> Fail, 0 -> Pass
    """
    log.info(run.__doc__)
    config = kw["config"]
    cephadm = CephAdmin(cluster=ceph_cluster, **config)
    rados_object = RadosOrchestrator(node=cephadm)
    scrub_object = RadosScrubber(node=cephadm)
    mon_obj = MonConfigMethods(rados_obj=rados_object)
    ceph_nodes = kw.get("ceph_nodes")
    osd_list = []
    total_osd_app_mem = {}
    osd_mem_pool = {}

    for node in ceph_nodes:
        if node.role == "osd":
            node_osds = rados_object.collect_osd_daemon_ids(node)
            osd_list = osd_list + node_osds

    target_configs = config["cache_trim_max_skip_pinned"]["configurations"]
    max_skip_pinned_value = int(
        mon_obj.get_config(section="osd", param="bluestore_cache_trim_max_skip_pinned")
    )

    # Check the default value of the  bluestore_cache_trim_max_skip_pinned value
    if max_skip_pinned_value != 1000:
        log.error(
            "The default value of  bluestore_cache_trim_max_skip_pinned not equal to 1000"
        )
        raise Exception(
            "The default value of bluestore_cache_trim_max_skip_pinned is not 1000"
        )

    # Creating pools and starting the test
    for entry in target_configs.values():
        log.debug(
            f"Creating {entry['pool_type']} pool on the cluster with name {entry['pool_name']}"
        )
        if entry.get("pool_type", "replicated") == "erasure":
            method_should_succeed(
                rados_object.create_erasure_pool, name=entry["pool_name"], **entry
            )
        else:
            method_should_succeed(
                rados_object.create_pool,
                **entry,
            )

        if not rados_object.bench_write(**entry):
            log.error("Failed to write objects into Pool")
            return 1
        rados_object.bench_read(**entry)
    log.info("Finished writing data into the pool")

    # performing scrub and deep-scrub
    rados_object.run_scrub()
    rados_object.run_deep_scrub()
    time.sleep(30)

    _, new_osd_list = rados_object.change_heap_profiler_state(osd_list, "start")
    # Executing tests for 45 minutes
    time_execution = datetime.datetime.now() + datetime.timedelta(minutes=45)
    while datetime.datetime.now() < time_execution:
        # Get all OSDs heap dump
        heap_dump = rados_object.get_heap_dump(new_osd_list)
        # get the osd application used memory
        osd_app_mem = get_bytes_used_by_app(heap_dump)
        total_osd_app_mem = mergeDictionary(total_osd_app_mem, osd_app_mem)
        # wait for 60 seconds and collecting the memory
        time.sleep(60)
    for osd_id, mem_list in total_osd_app_mem.items():
        mem_growth = is_what_percent_mem(mem_list)
        if mem_growth > 80:
            log.error(
                f"The osd.{osd_id} consuming more memory with the relative memory growth {mem_growth}"
            )
            raise Exception("No warning generated by PG Autoscaler")
        log.info(f"The relative memory growth for the osd.{osd_id} is {mem_growth} ")

    rados_object.change_heap_profiler_state(new_osd_list, "stop")
    execution_time = config["execution-time"]
    log.info(f"Checking osd memery growth for {execution_time} minutes")

    # Setting the scheduled scrub parameters to enable scrubbing continiously
    if scrub_object.get_osd_configuration("osd_scrub_begin_hour") != 0:
        scrub_object.set_osd_configuration("osd_scrub_begin_hour", 0)
        log.info("The osd_scrub_begin_hour is set to 0")
    if scrub_object.get_osd_configuration("osd_scrub_end_hour") != 0:
        scrub_object.set_osd_configuration("osd_scrub_end_hour", 0)
        log.info("The osd_scrub_end_hour is set to 0")
    if scrub_object.get_osd_configuration("osd_scrub_begin_week_day") != 0:
        scrub_object.set_osd_configuration("osd_scrub_begin_week_day", 0)
        log.info("The osd_scrub_begin_week_day is set to 0")
    if scrub_object.get_osd_configuration("osd_scrub_end_week_day") != 0:
        scrub_object.set_osd_configuration("osd_scrub_end_week_day", 0)
        log.info("The osd_scrub_end_week_day is set to 0")

    # Stroring the initial memory
    for osd in new_osd_list:
        memory = get_mempool_bytes(rados_object, osd)
        osd_mem_pool[osd] = memory

    # Intentional sleep
    time.sleep(int(execution_time) * 60)
    log.info(f"Intentional sleep for the {execution_time} minutes")

    # Checking the osd memory
    for osd in new_osd_list:
        new_memory = get_mempool_bytes(rados_object, osd)
        old_memory = osd_mem_pool.get(osd)
        log.info(f"The initial memory is :{old_memory}")
        log.info(f"The new memory is :{new_memory}")
        rel_mem_bytes = int(new_memory) - int(old_memory)
        # convert bytes to gb
        gb = rel_mem_bytes / (1024 * 1024 * 1024)
        log.info(f"The relative memory growth of OSD{osd} is {gb}")

        # Checking the OSD memory is increased or not while scrubbing
        if int(gb) > 2:
            msg = f"The memory for the osd.'{osd}' increasing and memory is greater than 2gb"
            log.error(msg)
            return 1

    # check fo the crashes in the cluster
    crash_list = rados_object.do_crash_ls()
    if not crash_list:
        return 0
    else:
        return 1


def get_mempool_bytes(rados_obj, osd_id):
    """
    To get the memory occupied by  the OSD
    Args:
         rados_obj : Rados object for executing ceph commands
         osd_id: osd id number
    Return:
          mem_bytes : Memory occuped by the osd in bytes
    """
    cmd = f"ceph tell osd.{osd_id} dump_mempools"
    out = rados_obj.run_ceph_command(cmd)
    mem_bytes = int(out["mempool"]["total"]["bytes"])
    return mem_bytes


def is_what_percent_mem(mem_list):
    """
    To calculate the relative memory growth
    Args:
         mem_list: list of mllac values of the application
    Return:
          result : Finds the relative growth of min amd max values

    """
    min_mem = int(min(mem_list))
    max_mem = int(max(mem_list))
    result = float(((max_mem - min_mem) * 100) / min_mem)
    return result


def get_bytes_used_by_app(osds_heap_dump):
    """
    To gets the bytes ued by the application in the heap dump
    Args:
         osds_heap_dump: List of osd heap dumps
         Example of heap dump:
              MALLOC:      429538736 (  409.6 MiB) Bytes in use by application
              MALLOC: +            0 (    0.0 MiB) Bytes in page heap freelist
              MALLOC: +      8934864 (    8.5 MiB) Bytes in central cache freelist
              MALLOC: +      3876864 (    3.7 MiB) Bytes in transfer cache freelist
              MALLOC: +     20636800 (   19.7 MiB) Bytes in thread cache freelists

    Return:
          osd_id_app_mem: Returns a dictionary with OSD id as a key and
                          malloc for the OSD application
                   Example: {1:['429538736' , '456789',..], 2:['21312','76576',..]...}
    """
    osd_id_app_mem = {}
    app_str = "Bytes in use by application"
    for osd_id, osd_dump in osds_heap_dump.items():
        malloc_str = osd_dump.split("MALLOC:")
        result = [app_dump for app_dump in malloc_str if app_str in app_dump]
        result = (" ".join(result)).strip()
        result_list = result.split("(")
        osd_id_app_mem[osd_id] = result_list[0].strip()
    return osd_id_app_mem


# Merge dictionaries
def mergeDictionary(dict_1, dict_2):
    """
    Merge two dictionaries by appending the first second dictionary values in to the first dictionary.
        Example1:
            dict_1 ={0:123,1:12,3:56}
            dict_2 ={0:9,1:45,3:77,4:78}
            Method returns dict_1 = {0:[123,9],1:[12,45],3:[56,77],4:78}
        Example2:
            dict_1 = {0:[123,9],1:[12,45],3:[56,77],4:78}
            dict_2 ={1:90,4:8,5:0}
            Method returns dict_1 = {0:[123,9],1:[12,45,90],3:[56,77],4:[78,8],5:0}
    Args:
        dict_1 & dict_2 : Two dictionaries
        Example:
            dict_1 ={0:123,1:12,3:56}
            dict_2 ={0:9,1:45,3:77,4:78}
    Return:
        dict_1 : Returns a dictionary after merging two dictionaries.
        Example:
            dict_1 = {0:[123,9],1:[12,45,90],3:[56,77],4:[78,8],5:0}
    """
    for key, value in dict_2.items():
        if key in dict_1:
            if isinstance(dict_1[key], list):
                dict_1[key].append(value)
            else:
                temp_list = [dict_1[key]]
                temp_list.append(value)
                dict_1[key] = temp_list
        else:
            dict_1[key] = value
    return dict_1
