tests:

  # Cluster deployment stage

  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                registry-url: registry.redhat.io
                mon-ip: node1
                initial-dashboard-password: admin@123
                dashboard-password-noupdate: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.1
              args:
                placement:
                  label: rgw
      desc: bootstrap with registry-url option and deployment services.
      destroy-cluster: false
      polarion-id: CEPH-83573713
      module: test_cephadm.py
      name: RHCS deploy cluster using cephadm

  - test:
      name: Monitoring Services deployment
      desc: Add monitoring services using spec file.
      module: test_cephadm.py
      polarion-id: CEPH-83574727
      config:
        steps:
          - config:
              command: apply_spec
              service: orch
              validate-spec-services: true
              specs:
                - service_type: prometheus
                  placement:
                    count: 1
                    nodes:
                      - node1
                - service_type: grafana
                  placement:
                    nodes:
                      - node1
                - service_type: alertmanager
                  placement:
                    count: 1
                - service_type: node-exporter
                  placement:
                    host_pattern: "*"
                - service_type: crash
                  placement:
                    host_pattern: "*"

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.1
        node: node6
        install_packages:
          - ceph-common
        copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758

  # Testing stage
  - test:
      config:
        script-name: test_manual_lc_process_single_bucket.py
        config-file-name: test_lc_process_single_bucket_expired.yaml
      desc: test LC process for a single bucket with expired objects
      module: sanity_rgw.py
      name: Test LC process for single bucket expired
      polarion-id: CEPH-83574809

  - test:
      config:
        script-name: test_manual_lc_process_single_bucket.py
        config-file-name: test_lc_process_single_bucket_nonexpired.yaml
      desc: test LC process for a single bucket with non expired objects
      module: sanity_rgw.py
      name: Test LC process for single bucket non-expired
      polarion-id: CEPH-83574809

  - test:
      name: Delete bucket after suspend is enabled
      desc: Delete bucket after suspend is enabled
      polarion-id: CEPH-9195
      module: sanity_rgw.py
      config:
        test-version: v2
        script-name: test_versioning_with_objects.py
        config-file-name: test_versioning_objects_suspended_delete.yaml

  - test:
      name: Get object and its version from same and different tenant users
      desc: test get object and its version from same and different tenant users
      polarion-id: CEPH-11516
      module: sanity_rgw.py
      config:
        test-version: v2
        script-name: test_bucket_policy_with_tenant_user.py
        config-file-name: get_object_and_its_versions_tenat_user.yaml
